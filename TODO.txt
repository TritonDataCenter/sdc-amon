# higher-level prios

- maintenance windows (MON-113):
    - regrok: alarm.handleEvent, app.handleMaintenanceEnd,
      maintenances.apiListAllMaintenanceWindows
    - add app.handleMaintenanceEnd
        - add 'reason' code field to `alarm.notify(...)` for the notification
          types to handle different reasons
    - tests for maintenance windows
    - separate commit: replace SuppressAlarmNotifications and UnsuppressAlarmNotifications
      with a maint window on that alarm's monitor
    - XXX's in maintenances.js
    - fix this app crash (when that alarm id is a 404):
        [root@headnode (bh1-kvm7:0) ~]# sdc-amon /pub/deadbeef-4444-4444-4444-444444444444/alarms/2 -X DELETE
        curl: (52) Empty reply from server
      traceback:
        node.js:201
                throw e; // process.nextTick error, or 'error' event on first tick
                      ^
        TypeError: Cannot set property 'faults' of null
            at /opt/smartdc/amon/lib/alarms.js:277:23
            at /opt/smartdc/amon/node_modules/redis/index.js:996:13


- ticket triage
- sysevent-based events: use case #2, machine-running-state probe type (?)
- agent in all zones
- MON-120: authorizeDelete finish (should merge with authorizePut logic)
- finish CRUD on UFDS objects
- sdc-healthcheck
- notification formatting: templating? Show open/closed state in notifications.
  Design section on what info should be included in notification.
- upgrade plan: What if UFDS schema needs to change? How do we rev the Master
  with older agents in the wild. Upgrading Agent versions? What about
  an old amon-agent? old amon-relay?
- cloudapi
- portal (c.f. http://circonus.com/resources/videos#play-video-check-creation)
- adminui
- load testing. Estimate of design-limit on number of
  probes/notifications/events
- HA



# todo (in no particular order)

- bitrot:
    - amon-master's manual ldap pooling: use ufds.js
    - restify
    - bunyan
    - node 0.8
- might want ssh in amon zone for dev work?
  josh: You can also enable ssh if you want, it's just a matter of importing a manifest for it and starting.
  kevin: https://stuff.joyent.us/stuff/joshw/ssh.xml
- dogfood:
    - devs using in kvm test machines
    - devs using in COAL
    - trevor using it in BH-1. For what exactly?
    - monitoring jenkins builds? or space usage in jenkins?
    - space usage on stuff
    - is DSAPI up?
    - is mo.jo up?
    - others? email around
- update to latest ldapjs for #49 and #50 fixes (already done I think)
- update to restify 1.4.1 when that is released (there is a ticket for this)
- update to and use npm shrinkwrap
- ldapjs with pooling when that is ready.
- removing old alarms after a week
- how to monitor percentiles? E.g. prio1 alarm if 90th percentile
  response time is >200ms or something?
- err cleanup (MON-108)
- Relay needs to have guards for run away agents. One case is an infinite
  loop: a probe watching for "ERROR" in amon logs, where sending the
  event results in an "ERROR". Handle this with relay per-agent throttling
  (can't use restify built-in throttling for this). If over limit, then
  an alarm is created for the monitor owner and the relay ignores or
  closes that zone server/socket.
- Consider adding run-time as attribute on probe events, i.e. how long it
  took the probe to run. Also, send a warning config alarm to a monitor owner
  if a probe is taking too long to run, e.g. a pathological regex. A *really*
  pathological regex (or mdb command) could hang and would need process
  mgmt to deal with. How to break out of this??? A separate process everytime
  is a pain.
- limits on number of objects per user: 1000 maint windows, 1000 monitors,
  1000 probes
- design use cases, assign a use case driver for each prio
    - use case #3: needs "smf" probe type to watch smf state transitions
      if possible
    - use case #6: needs a "mdb-kernel-cmd" or just a "cmd" probe type
      to periodically run a command.
    - use case #1: perhaps add 'smf-log-scan' probe type instead of
      giving the full service path. Important to periodically reget
      the smf log path via 'svcs -L' because it changes related to
      rotation or "file is open" snafus.
- hit up Ben for some "best practices for perf monitoring" info
  per https://mail.google.com/mail/u/1/?ui=2&shva=1#inbox/132dfa4e26d17478
- clear event scenario: A monitor with 'machine-up' probes on two zones.
  Both go down. One comes back up. That alarm should not clear, **but it
  does currently**.
- node-retry in PutEvents in RelayClient.
- 'sdc-amon /pub/hamish/monitors/whistle -X DELETE' fails
    ERROR: Error deleting 'amonmonitor=whistle, uuid=7b23ae63-37c9-420e-bb88-8d4bf5e30455, ou=customers, o=smartdc' from UFDS: NotAllowedOnNonLeafError:...
- What if a whole CN goes down? Could have heartbeats from the relays. Then
  have a probetype that fires for "missing" data. I.e. missed a heartbeat
  from the relay. Should this "fire on missing data" be a generic probe
  config thing? Probably.
  See <http://circonus.com/resources/videos#play-video-rule-notifications>
  for generic probe (aka rule) facilities: on match, invert match, on
  value change, on absence.
- <probe>.idObject details on probe events from agent is *wrong*. Insecure.
  The userUuid value must come from relay (and should be the zone owner-uuid
  cfg var). Relay should probably also add the "zone" from which the event
  came? Relay should also validate that given event is for a probe that
  *exists* for that zone, if not then drop it.
- consistent `new restify.FooError(...)` usage and document this
- send an op notification if Redis is down when attempting to process an event
- relay: improve this log line to know which zone/zsock this request came
  from. Perhaps the "unknown" could be the zone name. Also perhaps "anonymous"
  could be the owner uuid (only if no cost to that).
    unknown - anonymous [23/11/2011:21:38:26 GMT] "HEAD /agentprobes HTTP/1.1" 200 0 1
- disallow '/' in name fields (so can use that as sep char for a string id).
  If this isn't acceptable then update id'ification in agent.
  Perhaps ':' would be preferable for id'ification? Meh.
- name fields in Master API: percent escaping
  Also consider adding a "slug" field to Monitor and Probe. This is used as
  the 'id'. Guarantee uniqueness, export method (and library?) for calculating
  the slug. Still allow addressing by name? But perhaps as a query param?
  Think about it. The downside of "whatever goes" name as the only id is:
  inelegant alarm naming (use " 1" or "-1" suffix), potential for edge
  case errors up the stack (portal, cloudapi, adminui) all would have to
  get all the edge cases correct including unicode, PITA for using the API
  on the CLI (hand quoting URL parts), inelegant URLs in the portals
- walk through a server boot (and boot of all its zones): does Amon eventing
  go crazy?
- http://circonus.com/resources/videos#play-video-rule-notifications for
  probe facilities.
- probe types to consider: https://support.cloudkick.com/Category:Checks
  https://nodefly.com/


# someday/maybe

- long stack traces: https://github.com/mattinsler/longjohn
- nicer notification email formatting. Apple Store example: (trent's)
  https://mail.google.com/mail/u/1/?ui=2&shva=1#inbox/137c7a2c41219cee
    - see https://github.com/niftylettuce/node-email-templates and
      https://github.com/LearnBoost/juice
- more notification types: (see http://circonus.com/why-us/features/notifications)
    AOL and XMPP (Jabber, Google Talk, etc.) Instant Messages
    Twitter (direct messages from @circonusops)
    SMS
    PagerDuty
- saving the alarm events in redis? Are those needed for anything? Not
  bothering until we have UI that would use this.
- endpoint to test sending to a contact "POST /pub/:uuid/testnotify"
- restify v2.x
- "sdc-amon /pub/trent.mick" messes up on '.' in username -> restify upgrade
  Check for this being fixed with restify 1.x
- amon-relay and amon-agent: move node to build/node (a la portal and others)
- [Trent] master/config.json.in and use that in usb-headnode/zones/amon/setup
- from master/lib/probes.js:
    //XXX validate the type is an existing probe type
    //XXX validate data for that probe type
- [Trent] :login -> :uuid
- Amon test cases for alarming
- docs: move contact URN spec from master/lib/contact.js to the docs
- Idea: have amon-master create event/alarm to operator if it cannot
  connect/work with UFDS.
- look at New Relic: http://newrelic.com/
- test Amon on CN
- throttling on MonitorFakeFault
- [Trent] test suite for relay and agent
- 'ping-agent' probe type. Basically this is a test that the amon-relay <->
  amon-agent socket is working and that the agent is responding.
- sigar
- from workflow/mapi talk: Amon should have the monitor skel for provisioning
  workflow a la FWAPI (but still use normal Amon comm channel). Spec that.
- check interval: cloudkick checks run every 2.5 minutes.
- use case: modcloth request to have alerts when in CPU bursting range, i.e.
  they are hitting OS caps. In Modcloth's case, they were hitting caps and
  didn't know it.
- amon-relay just fails mostly silently if given agents-probes-dir doesn't exist:
      [15:40:37 trentm@banana:~/joy/amon/relay (ufds)]
      $ ../deps/node-install/bin/node main.js -v -d -D tmp/db -m http://127.0.0.1:8080 -s 8081 -p 90
      2011-11-10 23:40:43Z DEBUG: Checking master for new agent probes.
      2011-11-10 23:40:43Z DEBUG: Starting new amon-relay for global zone at "8081" (owner=joyent).
      2011-11-10 23:40:43Z WARN: unable to create staging area tmp/db/global: Error: ENOENT, No such file or directory 'tmp/db/global'
      2011-11-10 23:40:43Z DEBUG: Starting app on port 8081 (developer mode)
      2011-11-10 23:40:43Z INFO: Amon-relay listening in global zone at 8081.
      2011-11-10 23:40:43Z WARN: Unable to save new agent probes: Error: ENOENT, No such file or directory 'tmp/db/.6d617f32-4062-4bab-aa15-dfcf8f9e7113'
      2011-11-10 23:40:43Z INFO: Successfully updated agent probes from master (zone: global, md5: undefined -> UaTV7YW3MTyNMX+uguZZZw==).
- review <https://hub.joyent.com/wiki/display/dev/Operating+a+server+fleet>
  How are we doing?
- review and answer MVP points
- JSON api summary at "GET /"
- HTML docs at "GET /docs"
- SNMP traps (per Mark).
  http://tools.ietf.org/html/rfc3877  Alarm MIB
  http://tools.ietf.org/html/rfc3014  Notification Log MIB
  Traps not in snmpjs yet. Keith: """you can however use snmptrapgen, which
  comes with Net-SNMP on our systems it's just a binary program that
  generates traps and directs them somewhere of your choosing. So you can see
  what they look like. That may help you start writing the MIB you want to
  use (definitions of data objects, types, and structures)"""
- https://github.com/davepacheco/kang -ify
- edge-triggered alarms in master for probes
- CA probe type
- probe types: http://circonus.com/why-us/features/monitoring
- compare to http://www.rackspace.com/blog/cloud-monitoring-early-access-program-opens-its-doors/
  http://docs.rackspace.com/cm/api/v1.0/cm-devguide/content/index.html
- revive twilio notification backend as "sms"
- more probe types: run cmd, http check, zone list, ping, disk free, disk
  iops, cpu, mem, ps (aka "top")
    - http://blog.nodeping.com/2012/03/28/ssl-certificate-check/
- torture test on amon-relay resiliency on start
    // XXX Test/design for race condition where: get list of zones, zone
    //     goes down before zsock is created for it. I.e. should be buffering
    //     zoneevents until completed initial setup.
  Amon-relay restarts with lots of zones, while chaos-monkey hup'ing zones.
  Run that for a while then ensure that none of the zones are fubar'd with
  failures to shutdown.
- Interesting services for hooking into, messaging services that might be useful,
  or just general related/competing services:
    - http://aws.amazon.com/cloudwatch/
    - https://www.cloudkick.com/
    - http://www.splunk.com/
    - http://www.pagerduty.com/
    - http://www.opennms.org/ (modcloth using this I think)
    - circonus.com
    - newrelic


# notes: redis

- TODO: test redis being down and down/up/down/up frequently: does that break
  amon-master? It shouldn't.
- TODO: Test a notification going through even if redis is down.
- redis zone config: have "save 60 1". Make sure that isn't filling up disk
  with lots of snapshots. I *presume* this is just one or two files on
  disk, but should check.
- TODO: test how events are handled when redis runs out of memory
  Might want "maxmemory" and "maxmemory-policy ..." or "vm-enabled yes" in
  redis.conf


Ensure do "SELECT 1" (or some value of N) to stay indep of other users of
redis. TODO: Come up with a system for sharing this out. Suggest not using 0
such that any usage of it is an error to trap faulty users.



# Notes: suppression

(Note: Mostly this is, or should be, superceded by maintenance window
support. See design.restdown section for that.)

- suppression facility: global, per monitor, per customer? "per customer"
  could be implemented client side: portal/adminui just sets it for all
  that customer's monitors (looses mixed suppression states).

- suppressing some zone transition events: How?
  - the start + reboot for zone provisioning:
    - talk with Orlando about a "ready" zoneconfig var set by provisioner
  - the stop/start of an intentional reboot (by an operator at the GZ
    command line)
  - intentional reboot or shutdown in adminui
    - either adminui or mapi's shutdown/reboot command would call Amon
      API to suppress?
    - or this is a separate action in adminui to manually suppress
  - intentional reboot or shutdown in portal/cloudapi
    - if MAPI above, then this is done.
  - intentional reboot of the *compute node* for maintenance by operator
    - sdc-amon-suppress command, and/or equivalent in adminui UI.
  - sdc-amon-suppress:
    - one shot?
    - toggle?
    - what is scope? Low-level: per zone name and per monitor name. Optionally
      just per monitor name for all applicable zones?
        "Suppress this monitor [until enabled again later]."
        "Suppress this monitor for machines X, Y and Z."
        "Suppress all monitors for machines X, Y and Z."
      Or is this only about suppressing *alarms*? I.e. only at the master-level.
      Yes.
        "Suppress alarms for monitors M and N [until enabled again later]."
        "Suppress alarms for monitor M for N for machines X, Y, and Z."
        "... for the next hour."
      general:
        "Suppress alarms
          for monitors M and N (or all)
          for machines X, Y and Z (or all)
          for a certain amount of time (or until re-enabled)."


# notes: fma

Use case: operator wants an alert for every zone fault.

- FMA-based check in amon-agent in all CN GZs
  Q: Ask rm how to trigger that FMA event.
- Q: Exclude explicit reboots? What about an explicit reboot, but it doesn't
  come back up?
  A: Yes, exclude for now.
- Q: how to handle clearing an alarm. Does FMA give me an event for that?
  Ditto for non-fma-based zone status watching? Are we going to send an
  amon event for *every* zone running state every minute? No way.


# notes: smartos event streams notes

- sysevent: core element, syseventadm
  https://mo.joyent.com/source/xref/illumos-joyent/usr/src/lib/libsysevent/libsysevent.c#2653
    sysevent_subscribe_event
  Note that zonecfg_notify_* itself calls a slightly different "sysevent_evc_subscribe":
    https://mo.joyent.com/source/xref/illumos-joyent/usr/src/lib/libsysevent/libevchannel.c#566
    https://mo.joyent.com/source/xref/illumos-joyent/usr/src/lib/libzonecfg/common/libzonecfg.c#6692
- zonecfg_notify_bind: uses sysevent, might not need that
  defined here: illumos/usr/src/lib/libzonecfg/common/libzonecfg.c
- fma:
  fmdump
  FMA: http://hub.opensolaris.org/bin/download/Community+Group+fm/WebHome/FMDPRM.pdf
  http://download.oracle.com/docs/cd/E19082-01/819-3196/6n5ed4h40/index.html#indexterm-289
  fminject to test


# notes: events

restore this original sorta-schema for events from Mark?
The (sort of) schema for sending a status update:

      {
        "status": "<STATUS>",
        "message": "Free Form String.",
        "metrics": [{
          "name": "<URN>",
          "type": <TYPE>",
          "value": <VALUE>
        }];
      }

Where:

* <STATUS>: A string, that must be one of `"ok", "warn", "error"`.
* <URN>: The name of the check (note this will be validated against :check).
  An example is something like `urn:cpu:load`.
* <TYPE>: One of `"String", "Integer", "Boolean", "Float"`
* <VALUE>: Value for this metric.  Must correspond to `type`.

If not obvious, you can send multiple metrics per check.



# notes: severity

model Monitor:
  severity: ???

model Probe:
  severity: 1,2,3? circo. has 1-5.

Circonus has severity on the probe equiv.
