# higher-level prios

- back on 'machine-up' probe type:
  - TICKET for adding support for probes for down zones on startup
  - TICKET: tool to crash a zone: does zonecfg_notify event for leaving "running" get sent?
- put in readme/dev notes:

      PS1='\[\033[0;34m\][\u@\h (${DC_NAME}:${DC_HEADNODE_ID}) \w]\$\[\033[0m\] '

      [root@headnode (coal:0) /var/tmp]# cat admin-change.ldif
      dn: uuid=930896af-bf8c-48d4-885c-6573a94b1853, ou=users, o=smartdc
      changetype: modify
      replace: email
      email: trentm+coaladmin@gmail.com
      [root@headnode (coal:0) /var/tmp]# sdc-ldap modify -f admin-change.ldif
      ...

- missing pieces represented?
  - persistent alarms (and events) and API endpoints to work with open
    alarms (list, close, suppress alerts on this alarm)
  - "clearing" via persistent alarm
  - monitor suppression
  - agents running in every zone (and eventually in VMs)
  - Amon API in cloudapi and UIs in portal and adminui for working with Amon.
  - email improvements: Link to the alarm URL in the portal or adminui.
    Link to the machine in the portal or adminui.
  - More probe types
  - Experience, robustness.
- should get an email about that: put some basic templating in place
  for this:
      Subject: [{{cloudname}} 'sdczones' Monitor Alarm] 'cloudapi' machine is down

      Your 'sdczones' monitor has alarmed.

      Cloud: {{cloudname}}
      Monitor: {{monitorname}}
      Probe: {{probename}}
      Machine: {{machinename}}  (... extra details on the machine ...)

      --
      {{some kind of templatable stock footer}}
- update to and use npm shrinkwrap
- design use cases, assign a use case driver for each prio
- sysevent-based events: use case #2, machine-running-state probe type
- agent in all zones
- persistent alarms/events
  - see http://circonus.com/resources/api/rules
    'dashboard' option to show active alerts + those recnetly cleared
- authorizeDelete finish
- finish CRUD on UFDS objects
- sdc-healthcheck
- notification formatting
- suppression support
- upgrade plan: What if UFDS schema needs to change? How do we rev the Master
  with older agents in the wild. Upgrading Agent versions? What about
  an old amon-agent? old amon-relay?
- cloud api
- portal (c.f. http://circonus.com/resources/videos#play-video-check-creation)
- adminui
- load testing (hook into mock cloud for this?). Estimate of design-limit on
  number of probes/notifications/events
- HA


# todo (in no particular order)

- update to latest ldapjs for #49 and #50 fixes
- Relay needs to have guards for run away agents. One case is an infinite
  loop: a probe watching for "ERROR" in amon logs, where sending the
  event results in an "ERROR". Handle this with relay per-agent throttling
  (can't use restify built-in throttling for this). If over limit, then
  an alarm is created for the monitor owner and the relay ignores or
  closes that zone server/socket.
- Consider adding run-time as attribute on probe events, i.e. how long it
  took the probe to run. Also, send a warning config alarm to a monitor owner
  if a probe is taking too long to run, e.g. a pathological regex. A *really*
  pathological regex (or mdb command) could hang and would need process
  mgmt to deal with. How to break out of this??? A separate process everytime
  is a pain.
- 'sdc-amon /pub/hamish/monitors/whistle -X DELETE' fails
    ERROR: Error deleting 'amonmonitor=whistle, uuid=7b23ae63-37c9-420e-bb88-8d4bf5e30455, ou=customers, o=smartdc' from UFDS: NotAllowedOnNonLeafError:...
- What if a whole CN goes down? Could have heartbeats from the relays. Then
  have a probetype that fires for "missing" data. I.e. missed a heartbeat
  from the relay. Should this "fire on missing data" be a generic probe
  config thing? Probably.
- <probe>.idObject details on probe events from agent is *wrong*. Insecure.
  The userUuid value must come from relay (and should be the zone owner-uuid
  cfg var). Relay should probably also add the "zone" from which the event
  came? Relay should also validate that given event is for a probe that
  *exists* for that zone, if not then drop it.
- consistent `new restify.FooError(...)` usage and document this
- [Trent] master: improve this log line to include user context:
    2011-11-23 21:36:37Z DEBUG: App.processEvent: contact 'email' notified
- send an op notification if Redis is down when attempting to process an event
- relay: improve this log line to know which zone/zsock this request came
  from. Perhaps the "unknown" could be the zone name. Also perhaps "anonymous"
  could be the owner uuid (only if no cost to that).
    unknown - anonymous [23/11/2011:21:38:26 GMT] "HEAD /agentprobes HTTP/1.1" 200 0 1
- endpoint to test sending to a contact "POST /pub/:uuid/testnotify"
- disallow '/' in name fields (so can use that as sep char for a string id).
  If this isn't acceptable then update id'ification in agent.
  Perhaps ':' would be preferable for id'ification? Meh.
- name fields in Master API: percent escaping
- throttling on TestMonitorNotify
- walk through a server boot (and boot of all its zones): does Amon eventing
  go crazy?
- notification types to consider (http://circonus.com/why-us/features/notifications):
    Email
    AOL and XMPP (Jabber, Google Talk, etc.) Instant Messages
    Twitter (direct messages from @circonusops)
    SMS
    HTTP WebHooks


# someday/maybe

- "sdc-amon /pub/trent.mick" messes up on '.' in username -> restify upgrade
  Check for this being fixed with restify 1.x
- [Trent] master/config.json.in and use that in usb-headnode/zones/amon/setup
- from master/lib/probes.js:
    //XXX validate the type is an existing probe type
    //XXX validate data for that probe type
- [Trent] :login -> :uuid
- docs: move contact URN spec from master/lib/contact.js to the docs
- Idea: have amon-master create event/alarm to operator if it cannot
  connect/work with UFDS.
- [Trent] test suite for relay and agent
- 'ping-agent' probe type. Basically this is a test that the amon-relay <->
  amon-agent socket is working and that the agent is responding.
- from workflow/mapi talk: Amon should have the monitor skel for provisioning
  workflow a la FWAPI (but still use normal Amon comm channel). Spec that.
- use case: modcloth request to have alerts when in CPU bursting range, i.e.
  they are hitting OS caps. In Modcloth's case, they were hitting caps and
  didn't know it.
- amon-relay just fails mostly silently if given agents-probes-dir doesn't exist:
      [15:40:37 trentm@banana:~/joy/amon/relay (ufds)]
      $ ../deps/node-install/bin/node main.js -v -d -D tmp/db -m http://127.0.0.1:8080 -s 8081 -p 90
      2011-11-10 23:40:43Z DEBUG: Checking master for new agent probes.
      2011-11-10 23:40:43Z DEBUG: Starting new amon-relay for global zone at "8081" (owner=joyent).
      2011-11-10 23:40:43Z WARN: unable to create staging area tmp/db/global: Error: ENOENT, No such file or directory 'tmp/db/global'
      2011-11-10 23:40:43Z DEBUG: Starting app on port 8081 (developer mode)
      2011-11-10 23:40:43Z INFO: Amon-relay listening in global zone at 8081.
      2011-11-10 23:40:43Z WARN: Unable to save new agent probes: Error: ENOENT, No such file or directory 'tmp/db/.6d617f32-4062-4bab-aa15-dfcf8f9e7113'
      2011-11-10 23:40:43Z INFO: Successfully updated agent probes from master (zone: global, md5: undefined -> UaTV7YW3MTyNMX+uguZZZw==).
- review <https://hub.joyent.com/wiki/display/dev/Operating+a+server+fleet>
  How are we doing?
- JSON api summary at "GET /"
- HTML docs at "GET /docs"
- SNMP traps (per Mark).
  http://tools.ietf.org/html/rfc3877  Alarm MIB
  http://tools.ietf.org/html/rfc3014  Notification Log MIB
  Traps not in snmpjs yet. Keith: """you can however use snmptrapgen, which
  comes with Net-SNMP on our systems it's just a binary program that
  generates traps and directs them somewhere of your choosing. So you can see
  what they look like. That may help you start writing the MIB you want to
  use (definitions of data objects, types, and structures)"""

- edge-triggered alarms in master for probes
- CA probe type
- HTTP probe type
- probe types: http://circonus.com/why-us/features/monitoring
- revive twilio notification backend as "sms"
- more probe types: run cmd, http check, zone list, ping, disk free, disk
  iops, cpu, mem, ps (aka "top")
- torture test on amon-relay resiliency on start
    // XXX Test/design for race condition where: get list of zones, zone
    //     goes down before zsock is created for it. I.e. should be buffering
    //     zoneevents until completed initial setup.
  Amon-relay restarts with lots of zones, while chaos-monkey hup'ing zones.
  Run that for a while then ensure that none of the zones are fubar'd with
  failures to shutdown.
- Interesting services for hooking into, messaging services that might be useful,
  or just general related/competing services:
    - http://aws.amazon.com/cloudwatch/
    - https://www.cloudkick.com/
    - http://www.splunk.com/
    - http://www.pagerduty.com/
    - http://www.opennms.org/ (modcloth using this I think)



# Notes: redis

- redis zone config: have "save 60 1". Make sure that isn't filling up disk
  with lots of snapshots. I *presume* this is just one or two files on
  disk, but should check.
- test: test how events are handled when redis runs out of memory
  Might want "maxmemory" and "maxmemory-policy ..." or "vm-enabled yes" in
  redis.conf


# Notes: suppression

- suppression facility: global, per monitor, per customer? "per customer"
  could be implemented client side: portal/adminui just sets it for all
  that customer's monitors (looses mixed suppression states).

- suppressing some zone transition events: How?
  - the start + reboot for zone provisioning:
    - talk with Orlando about a "ready" zoneconfig var set by provisioner
  - the stop/start of an intentional reboot (by an operator at the GZ
    command line)
    - sdc-amon-suppress ...   # or some command line that
  - intentional reboot or shutdown in adminui
    - either adminui or mapi's shutdown/reboot command would call Amon
      API to suppress?
    - or this is a separate action in adminui to manually suppress
  - intentional reboot or shutdown in portal/cloudapi
    - if MAPI above, then this is done.
  - intentional reboot of the *compute node* for maintenance by operator
    - sdc-amon-suppress command, and/or equivalent in adminui UI.
  - sdc-amon-suppress:
    - one shot?
    - toggle?
    - what is scope? Low-level: per zone name and per monitor name. Optionally
      just per monitor name for all applicable zones?
        "Suppress this monitor [until enabled again later]."
        "Suppress this monitor for machines X, Y and Z."
        "Suppress all monitors for machines X, Y and Z."
      Or is this only about suppressing *alarms*? I.e. only at the master-level.
      Yes.
        "Suppress alarms for monitors M and N [until enabled again later]."
        "Suppress alarms for monitor M for N for machines X, Y, and Z."
        "... for the next hour."
      general:
        "Suppress alarms
          for monitors M and N (or all)
          for machines X, Y and Z (or all)
          for a certain amount of time (or until re-enabled)."


# notes: fma

Use case: operator wants an alert for every zone fault.

- FMA-based check in amon-agent in all CN GZs
  Q: Ask rm how to trigger that FMA event.
- Q: Exclude explicit reboots? What about an explicit reboot, but it doesn't
  come back up?
  A: Yes, exclude for now.
- Q: how to handle clearing an alarm. Does FMA give me an event for that?
  Ditto for non-fma-based zone status watching? Are we going to send an
  amon event for *every* zone running state every minute? No way.


# notes: smartos event streams notes

- sysevent: core element, syseventadm
  https://mo.joyent.com/source/xref/illumos-joyent/usr/src/lib/libsysevent/libsysevent.c#2653
    sysevent_subscribe_event
  Note that zonecfg_notify_* itself calls a slightly different "sysevent_evc_subscribe":
    https://mo.joyent.com/source/xref/illumos-joyent/usr/src/lib/libsysevent/libevchannel.c#566
    https://mo.joyent.com/source/xref/illumos-joyent/usr/src/lib/libzonecfg/common/libzonecfg.c#6692
- zonecfg_notify_bind: uses sysevent, might not need that
  defined here: illumos/usr/src/lib/libzonecfg/common/libzonecfg.c
- fma:
  fmdump
  FMA: http://hub.opensolaris.org/bin/download/Community+Group+fm/WebHome/FMDPRM.pdf
  http://download.oracle.com/docs/cd/E19082-01/819-3196/6n5ed4h40/index.html#indexterm-289
  fminject to test


# notes: events

restore this original sorta-schema for events from Mark?
The (sort of) schema for sending a status update:

      {
        "status": "<STATUS>",
        "message": "Free Form String.",
        "metrics": [{
          "name": "<URN>",
          "type": <TYPE>",
          "value": <VALUE>
        }];
      }

Where:

* <STATUS>: A string, that must be one of `"ok", "warn", "error"`.
* <URN>: The name of the check (note this will be validated against :check).
  An example is something like `urn:cpu:load`.
* <TYPE>: One of `"String", "Integer", "Boolean", "Float"`
* <VALUE>: Value for this metric.  Must correspond to `type`.

If not obvious, you can send multiple metrics per check.


Current event layouts (working on a spec for this). Event from logscan
probe type:

    {
      "v":1,
      "probe":{
          "user":"44444444-4444-4444-4444-444444444444",
          "monitor":"gz",
          "name":"smartlogin",
          "type":"logscan"
      },
      "time":"2012-03-07T01:01:19.963Z",
      "data":{
          "message":"Log \"/var/svc/log/smartdc-agent-smartlogin:default.log\" matched /Stopping/.",
          "value":1,
          "details":{
              "match":"[ Mar 7 01:01:19 Stopping because service restarting. ]\n[ Mar 7 01:01:19 Executing stop method (:kill). ]"
          }
      },
      "machine": UUID
      // Added by relay:
      "machine": <will override if from a machine>,
      "server":"564d986b-793c-9595-e30a-36b06215d7a4",
      "uuid":"1e5933ba-c813-44fc-bb54-8b75ebf53eff"
    }

Fake event from /pub/:user/monitors/:monitor/:testnotify:

    {
      v: 1,
      probe: {
        user: req._user.uuid,
        monitor: monitor.name,
        name: "test",
        type: "test"
      },
      time: new Date(),
      data:{
        message: "Test notification.",
      },
      uuid: uuid()
    }




# notes: alarms

- missing monitor and alarm *priority* or severity

model Alarm:  (a)
    uuid:
    timestamp:
    priority: Or could just inherit this from `monitor`
    state|suppressed
    dupeOf: used for grouping de-dupes
    watch: uuid of watch that generted this
    zone: zone id if came from a zone
