# higher-level prios

- test suites on staging (including adding for alarms) QA-101
- design use cases, assign a use case driver for each prio
- sysevent-based events: use case #2, machine-running-state probe type
- agent in all zones
- authorizeDelete finish (should merge with authorizePut logic)
- finish CRUD on UFDS objects
- sdc-healthcheck
- notification formatting: templating? Show open/closed state in notifications.
  Design section on what info should be included in notification.
- suppression support (alarms, monitors, everything)
- upgrade plan: What if UFDS schema needs to change? How do we rev the Master
  with older agents in the wild. Upgrading Agent versions? What about
  an old amon-agent? old amon-relay?
- cloudapi
- portal (c.f. http://circonus.com/resources/videos#play-video-check-creation)
- adminui
- load testing (hook into mock cloud for this?). Estimate of design-limit on
  number of probes/notifications/events
- HA


# todo (in no particular order)

- update to latest ldapjs for #49 and #50 fixes
- update others to restify 1.1
- update to and use npm shrinkwrap
- Relay needs to have guards for run away agents. One case is an infinite
  loop: a probe watching for "ERROR" in amon logs, where sending the
  event results in an "ERROR". Handle this with relay per-agent throttling
  (can't use restify built-in throttling for this). If over limit, then
  an alarm is created for the monitor owner and the relay ignores or
  closes that zone server/socket.
- Consider adding run-time as attribute on probe events, i.e. how long it
  took the probe to run. Also, send a warning config alarm to a monitor owner
  if a probe is taking too long to run, e.g. a pathological regex. A *really*
  pathological regex (or mdb command) could hang and would need process
  mgmt to deal with. How to break out of this??? A separate process everytime
  is a pain.
- clear event scenario: A monitor with 'machine-up' probes on two zones.
  Both go down. One comes back up. That alarm should not clear, **but it
  does currently**.
- node-retry in PutEvents in RelayClient.
- 'sdc-amon /pub/hamish/monitors/whistle -X DELETE' fails
    ERROR: Error deleting 'amonmonitor=whistle, uuid=7b23ae63-37c9-420e-bb88-8d4bf5e30455, ou=customers, o=smartdc' from UFDS: NotAllowedOnNonLeafError:...
- What if a whole CN goes down? Could have heartbeats from the relays. Then
  have a probetype that fires for "missing" data. I.e. missed a heartbeat
  from the relay. Should this "fire on missing data" be a generic probe
  config thing? Probably.
  See <http://circonus.com/resources/videos#play-video-rule-notifications>
  for generic probe (aka rule) facilities: on match, invert match, on
  value change, on absence.
- <probe>.idObject details on probe events from agent is *wrong*. Insecure.
  The userUuid value must come from relay (and should be the zone owner-uuid
  cfg var). Relay should probably also add the "zone" from which the event
  came? Relay should also validate that given event is for a probe that
  *exists* for that zone, if not then drop it.
- consistent `new restify.FooError(...)` usage and document this
- send an op notification if Redis is down when attempting to process an event
- relay: improve this log line to know which zone/zsock this request came
  from. Perhaps the "unknown" could be the zone name. Also perhaps "anonymous"
  could be the owner uuid (only if no cost to that).
    unknown - anonymous [23/11/2011:21:38:26 GMT] "HEAD /agentprobes HTTP/1.1" 200 0 1
- disallow '/' in name fields (so can use that as sep char for a string id).
  If this isn't acceptable then update id'ification in agent.
  Perhaps ':' would be preferable for id'ification? Meh.
- name fields in Master API: percent escaping
  Also consider adding a "slug" field to Monitor and Probe. This is used as
  the 'id'. Guarantee uniqueness, export method (and library?) for calculating
  the slug. Still allow addressing by name? But perhaps as a query param?
  Think about it. The downside of "whatever goes" name as the only id is:
  inelegant alarm naming (use " 1" or "-1" suffix), potential for edge
  case errors up the stack (portal, cloudapi, adminui) all would have to
  get all the edge cases correct including unicode, PITA for using the API
  on the CLI (hand quoting URL parts), inelegant URLs in the portals
- walk through a server boot (and boot of all its zones): does Amon eventing
  go crazy?
- http://circonus.com/resources/videos#play-video-rule-notifications for
  probe facilities.
- probe types to consider: https://support.cloudkick.com/Category:Checks


# someday/maybe

- more notification types: (see http://circonus.com/why-us/features/notifications)
    AOL and XMPP (Jabber, Google Talk, etc.) Instant Messages
    Twitter (direct messages from @circonusops)
    SMS
    PagerDuty
- saving the alarm events in redis? Are those needed for anything? Not
  bothering until we have UI that would use this.
- endpoint to test sending to a contact "POST /pub/:uuid/testnotify"
- "sdc-amon /pub/trent.mick" messes up on '.' in username -> restify upgrade
  Check for this being fixed with restify 1.x
- amon-relay and amon-agent: move node to build/node (a la portal and others)
- [Trent] master/config.json.in and use that in usb-headnode/zones/amon/setup
- from master/lib/probes.js:
    //XXX validate the type is an existing probe type
    //XXX validate data for that probe type
- [Trent] :login -> :uuid
- docs: move contact URN spec from master/lib/contact.js to the docs
- Idea: have amon-master create event/alarm to operator if it cannot
  connect/work with UFDS.
- throttling on MonitorFakeFault
- [Trent] test suite for relay and agent
- 'ping-agent' probe type. Basically this is a test that the amon-relay <->
  amon-agent socket is working and that the agent is responding.
- from workflow/mapi talk: Amon should have the monitor skel for provisioning
  workflow a la FWAPI (but still use normal Amon comm channel). Spec that.
- check interval: cloudkick checks run every 2.5 minutes.
- use case: modcloth request to have alerts when in CPU bursting range, i.e.
  they are hitting OS caps. In Modcloth's case, they were hitting caps and
  didn't know it.
- amon-relay just fails mostly silently if given agents-probes-dir doesn't exist:
      [15:40:37 trentm@banana:~/joy/amon/relay (ufds)]
      $ ../deps/node-install/bin/node main.js -v -d -D tmp/db -m http://127.0.0.1:8080 -s 8081 -p 90
      2011-11-10 23:40:43Z DEBUG: Checking master for new agent probes.
      2011-11-10 23:40:43Z DEBUG: Starting new amon-relay for global zone at "8081" (owner=joyent).
      2011-11-10 23:40:43Z WARN: unable to create staging area tmp/db/global: Error: ENOENT, No such file or directory 'tmp/db/global'
      2011-11-10 23:40:43Z DEBUG: Starting app on port 8081 (developer mode)
      2011-11-10 23:40:43Z INFO: Amon-relay listening in global zone at 8081.
      2011-11-10 23:40:43Z WARN: Unable to save new agent probes: Error: ENOENT, No such file or directory 'tmp/db/.6d617f32-4062-4bab-aa15-dfcf8f9e7113'
      2011-11-10 23:40:43Z INFO: Successfully updated agent probes from master (zone: global, md5: undefined -> UaTV7YW3MTyNMX+uguZZZw==).
- review <https://hub.joyent.com/wiki/display/dev/Operating+a+server+fleet>
  How are we doing?
- JSON api summary at "GET /"
- HTML docs at "GET /docs"
- SNMP traps (per Mark).
  http://tools.ietf.org/html/rfc3877  Alarm MIB
  http://tools.ietf.org/html/rfc3014  Notification Log MIB
  Traps not in snmpjs yet. Keith: """you can however use snmptrapgen, which
  comes with Net-SNMP on our systems it's just a binary program that
  generates traps and directs them somewhere of your choosing. So you can see
  what they look like. That may help you start writing the MIB you want to
  use (definitions of data objects, types, and structures)"""
- https://github.com/davepacheco/kang -ify
- edge-triggered alarms in master for probes
- CA probe type
- probe types: http://circonus.com/why-us/features/monitoring
- compare to http://www.rackspace.com/blog/cloud-monitoring-early-access-program-opens-its-doors/
  http://docs.rackspace.com/cm/api/v1.0/cm-devguide/content/index.html
- revive twilio notification backend as "sms"
- more probe types: run cmd, http check, zone list, ping, disk free, disk
  iops, cpu, mem, ps (aka "top")
    - http://blog.nodeping.com/2012/03/28/ssl-certificate-check/
- torture test on amon-relay resiliency on start
    // XXX Test/design for race condition where: get list of zones, zone
    //     goes down before zsock is created for it. I.e. should be buffering
    //     zoneevents until completed initial setup.
  Amon-relay restarts with lots of zones, while chaos-monkey hup'ing zones.
  Run that for a while then ensure that none of the zones are fubar'd with
  failures to shutdown.
- Interesting services for hooking into, messaging services that might be useful,
  or just general related/competing services:
    - http://aws.amazon.com/cloudwatch/
    - https://www.cloudkick.com/
    - http://www.splunk.com/
    - http://www.pagerduty.com/
    - http://www.opennms.org/ (modcloth using this I think)
    - circonus.com
    - newrelic


# notes: redis

- TODO: test redis being down and down/up/down/up frequently: does that break
  amon-master? It shouldn't.
- TODO: Test a notification going through even if redis is down.
- redis zone config: have "save 60 1". Make sure that isn't filling up disk
  with lots of snapshots. I *presume* this is just one or two files on
  disk, but should check.
- TODO: test how events are handled when redis runs out of memory
  Might want "maxmemory" and "maxmemory-policy ..." or "vm-enabled yes" in
  redis.conf


Ensure do "SELECT 1" (or some value of N) to stay indep of other users of
redis. TODO: Come up with a system for sharing this out. Suggest not using 0
such that any usage of it is an error to trap faulty users.



# Notes: suppression

- suppression facility: global, per monitor, per customer? "per customer"
  could be implemented client side: portal/adminui just sets it for all
  that customer's monitors (looses mixed suppression states).

- suppressing some zone transition events: How?
  - the start + reboot for zone provisioning:
    - talk with Orlando about a "ready" zoneconfig var set by provisioner
  - the stop/start of an intentional reboot (by an operator at the GZ
    command line)
  - intentional reboot or shutdown in adminui
    - either adminui or mapi's shutdown/reboot command would call Amon
      API to suppress?
    - or this is a separate action in adminui to manually suppress
  - intentional reboot or shutdown in portal/cloudapi
    - if MAPI above, then this is done.
  - intentional reboot of the *compute node* for maintenance by operator
    - sdc-amon-suppress command, and/or equivalent in adminui UI.
  - sdc-amon-suppress:
    - one shot?
    - toggle?
    - what is scope? Low-level: per zone name and per monitor name. Optionally
      just per monitor name for all applicable zones?
        "Suppress this monitor [until enabled again later]."
        "Suppress this monitor for machines X, Y and Z."
        "Suppress all monitors for machines X, Y and Z."
      Or is this only about suppressing *alarms*? I.e. only at the master-level.
      Yes.
        "Suppress alarms for monitors M and N [until enabled again later]."
        "Suppress alarms for monitor M for N for machines X, Y, and Z."
        "... for the next hour."
      general:
        "Suppress alarms
          for monitors M and N (or all)
          for machines X, Y and Z (or all)
          for a certain amount of time (or until re-enabled)."


# notes: fma

Use case: operator wants an alert for every zone fault.

- FMA-based check in amon-agent in all CN GZs
  Q: Ask rm how to trigger that FMA event.
- Q: Exclude explicit reboots? What about an explicit reboot, but it doesn't
  come back up?
  A: Yes, exclude for now.
- Q: how to handle clearing an alarm. Does FMA give me an event for that?
  Ditto for non-fma-based zone status watching? Are we going to send an
  amon event for *every* zone running state every minute? No way.


# notes: smartos event streams notes

- sysevent: core element, syseventadm
  https://mo.joyent.com/source/xref/illumos-joyent/usr/src/lib/libsysevent/libsysevent.c#2653
    sysevent_subscribe_event
  Note that zonecfg_notify_* itself calls a slightly different "sysevent_evc_subscribe":
    https://mo.joyent.com/source/xref/illumos-joyent/usr/src/lib/libsysevent/libevchannel.c#566
    https://mo.joyent.com/source/xref/illumos-joyent/usr/src/lib/libzonecfg/common/libzonecfg.c#6692
- zonecfg_notify_bind: uses sysevent, might not need that
  defined here: illumos/usr/src/lib/libzonecfg/common/libzonecfg.c
- fma:
  fmdump
  FMA: http://hub.opensolaris.org/bin/download/Community+Group+fm/WebHome/FMDPRM.pdf
  http://download.oracle.com/docs/cd/E19082-01/819-3196/6n5ed4h40/index.html#indexterm-289
  fminject to test


# notes: events

restore this original sorta-schema for events from Mark?
The (sort of) schema for sending a status update:

      {
        "status": "<STATUS>",
        "message": "Free Form String.",
        "metrics": [{
          "name": "<URN>",
          "type": <TYPE>",
          "value": <VALUE>
        }];
      }

Where:

* <STATUS>: A string, that must be one of `"ok", "warn", "error"`.
* <URN>: The name of the check (note this will be validated against :check).
  An example is something like `urn:cpu:load`.
* <TYPE>: One of `"String", "Integer", "Boolean", "Float"`
* <VALUE>: Value for this metric.  Must correspond to `type`.

If not obvious, you can send multiple metrics per check.


Current event layouts (working on a spec for this). Event from logscan
probe type (a "probe event"):

    {
      "v":1,
      "time":1331782620613,
      "type": "probe",
      "user": "44444444-4444-4444-4444-444444444444",
      "monitor": "gz",
      "probe": "smartlogin",
      "probeType": "logscan"
      "clear": false,
      "data":{
        "message":"Log \"/var/svc/log/smartdc-agent-smartlogin:default.log\" matched /Stopping/.",
        "value":1,
        "details":{
          "match":"[ Mar 7 01:01:19 Stopping because service restarting. ]\n[ Mar 7 01:01:19 Executing stop method (:kill). ]"
        }
      },
      "machine": UUID
      // Added by relay:
      "machine": <will override if from a machine>,
      "server":"564d986b-793c-9595-e30a-36b06215d7a4",
      "uuid":"1e5933ba-c813-44fc-bb54-8b75ebf53eff"
    }

Test event from "POST /pub/:user/monitors/:monitor?action=fakefault"
(a "test event" or a "monitor event", i.e. associated with a monitor):

    {
      v: 1,
      time: Date.now(),
      type: "fake",
      user: UUID,
      monitor: NAME
      clear: CLEAR,
      data:{
        message: "Test notification.",
      },
      uuid: uuid()
    }

Other potential event types:

- "operator event": send to (Amon) operator for some internal Amon problem
- "user event": send to user for some configuration problem in their data?
  E.g. I was think that with groups, notify the group owner if can't contact
  one of the group members. Not sure though.
- "relay" event? A problem report from a relay? Perhaps this is just an
  operator event? Not sure.


# notes: severity

model Monitor:
  severity: ???

model Probe:
  severity: 1,2,3? circo. has 1-5.

Circonus has severity on the probe equiv.


# notes: alarms

model Alarm:
    user: UUID
    monitor: name, might be null
    name: this is the id for the alarm, see "Alarm id" section below
    timeOpened: when first alarmed
    openedDuringMaint: null or ref to maintenance window? Or just true|false.
    timeClosed: when cleared (auto or explcitly)
    timeLastActivity: (If useful.) This is time of last event or API action
      on this alarm. Perhaps last notification?
    timeLastEvent: Used for de-duping. This is a bit of denorm from `events`
      field.
    timeExpiry: set to N (N == 1 week) after timeClosed whenever it is closed
      This should be useful for portals to show when this will expire.
    suppressNotifications: true|false
    severity: Or could just inherit this from monitor/probe
    isOpen: true|false  (s/isOpen/closed/)
    probes: the *set* of probe *names* from this monitor that have tripped.
    machines: A probe can change, so really want the list of machines
      affected (or really the ones that ran the tripped probes).
    events: ... can't store all events for this so perhaps:
      - firstEvents, recentEvents (N events on either end), numEvents (total)
        Perhaps "N" is 50 here, i.e. something high enough to not be
        common.
      - might want some sort of histogram of events. Can we store a
        timestamp for every event? *Can* we store all events? Not really.
        Can redis help us here?
    numNotifications
      - perhaps also time of last notification? time of first notification.


probe event
  -> handleEvent(event, monitor /* might be null */, probe /* might be null */)
  -> getOrCreateAlarm(event, monitor, probe)  // monitor and probe might be null
      i.e. want alarmFromMonitor ...
      Is there only ever one alarm for a monitor? What if there is an old
      stale (no events for a long time) unclosed alarm for monitor M, then
      a new event comes in? Either is revives that alarm or it creates
      a new one. -> new one (unique names include closed monitors).
      So need threshold on when to break to a new alarm.
      Sep this out to a function that decides of this event "is related"
      to this alarm. For starters this is just whether
      `event.time - alarm.timeLastEvent > 1 hour`. Perhaps expose that
      threshold to the monitor config (`monitor.dedupePeriod`). Later the
      algo could be adaptive or could look at other params (e.g. same probe?
      same zone? similar period of events, i.e. every day at midnight?)
  -> alarm.notify(event, monitor, probe) ?
      There is the first notify, on alarm creation.
      - Modulo "delay" on first notification for transient errors. See
        zabbix and nagios options below.
        "[x] Don't notify on transient error.  Period: [  60] seconds."

      Subsequent events on the same alarm:
      - Dumbest is to notify every time.
      - Notify no more than once per N minutes.
      - Notify every five minutes regardless of additional events.
      - Notify again if higher severity event.
      - ...
      - Zabbix escalation? Don't go there.
      - http://www.zabbix.com/wiki/howto/config/alerts/delaying_notifications
        Nagios "soft states".
        Delay first notification for transient problems. Would NOT want
        for a reboot, but perhaps for a disk size threshold.

      TODO: Email to ops (linda, ben) on what kind of notification and
        re-notification setup they use in prod. If there is anything they'd
        prefer to see. E.g. for repeated failures of a particular
        check/monitor.

      These are attributes of the monitor. What about default? On the user?

      Implementation. How to handle scheduling and reliable
      sending of notifications?
      - put them in redis, have a interval that checks every N seconds for
        a notification to send (perhaps with a .kick() to send right away)
      - actually want to setup a ping for the alarm to see about notifying.
        So an alarm could handle this itself with setTimeout. If it is to
        be delayed, then it is up to that Alarm instance to put it in redis
        in case of restarts.

      So not `alarm.notify(...)` but `alarm.start()`. Is it too much to have
      an alarm in memory the whole time? Yes. So not managed by each alarm
      instance.

      Could it be "active alarms"? I.e. reminder notifications time out
      if not new events on an alarm. Or just not support reminder
      notifications (for now). Punt on "delay for transient" failure for now
      too. That means no timer needed.

Implementation (take 2):
probe event
  -> app.processEvent(event)
    Get `monitor` and `probe`.
    Get or create new Alarm: alarm.
    app.attachEventToAlarm(alarm, event, ...)
  -> app.getOrCreateAlarm(event, user, monitor, probe, function (err, alarm))
    - Get all open alarms for this user/monitor.
    - If not any -> create new alarm.
    - Pass all potential alarms to app.chooseRelatedAlarm().
    - If alarm -> return alarm
    - Else -> create new alarm.
  -> app.chooseRelatedAlarm(alarms, event, user, monitor, probe, function(err, alarm))
    First pass at this: Choose the alarm with the most recent `timeLastEvent`.
    If `event.time - alarm.timeLastEvent > 1 hour` then return none, i.e.
    not related. Else, return that alarm.
    Eventually make this "1 hour" an optional var on monitor.
    Eventually this algo can consider more vars.
  -> app.createAlarm(alarm, event, monitor)
    uuid: uuid()
    monitor:
  -> app.alarmAddEvent(alarm, event, ...)
    - update alarm fields:
        timeOpened: when first alarmed
        timeLastActivity: (If useful.) This is time of last event or API action
          on this alarm. Perhaps last notification?
        timeLastEvent: Used for de-duping. This is a bit of denorm from `events`
          field.
        severity: Or could just inherit this from `monitor`
        isOpen: true|false
        probes: the *set* of probe *names* from this monitor that have tripped.
        machines: A probe can change, so really want the list of machines
          affected (or really the ones that ran the tripped probes).
        events: ... can't store all events for this so perhaps:
          - firstEvents, recentEvents (N events on either end), numEvents (total)
            Perhaps "N" is 50 here, i.e. something high enough to not be
            common.
          - might want some sort of histogram of events. Can we store a
            timestamp for every event? *Can* we store all events? Not really.
            Can redis help us here?
    - Decide whether to notify:
      - if this is a clear event (XXX probe event needs a 'clear: true'
        field, set by machine-up 'up' event) and never opened, then no
      - if in maint, then no (update 'openedDuringMaint')
      - ... more?
      - else, notify:
          numNotifications



.
