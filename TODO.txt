# MVP

"The absolute MVP for Monitoring is having the ability to alert when a
VM or Zone goes down, and the ability to alert someone via email." -- Trevor

More specific MVP:
- Amon is up an running in a default SDC setup (modulo headnode-refactor work).
  Details: 'amon' headnode zone running amon-master. 'amonriak' headnode
  zone. 'amon-relay' running in each compute node GZ.
  Q: Unsure whether getting an amon-agent running in each zone is necessary.
  A: If only requirement is zone/vm start/stop, then don't need agent in zone.
  Q: Also what about amon-agent in VMs? That would have to be optional and not feasible for Windows right now.
  A: Punt for SDC 6.5.
- Amon can be setup to email an operator when a VM or Zone goes down.
  Q: distinguishing between intentional reboots and shutdowns?
  A:
  Q: required to guard against email flood on *server* shutdown?
  A:
- Docs on adding more amonriak nodes.
- Docs on setting up the notifications above.

Internal requirements for first release:
- Upgradeable:
  - db migration story
  - amon-master upgrade story
  - amon-relay upgrade story
  - amon-agent install/upgrade story
- Test suite with decent coverage.
- Load testing.


# SDC 6.5

Remaining days. Fixed holidays, milestones and non-amon work "[in brackets]".
Proposed target functionality/work for the other days. Note that John has
offered to do some work on amon if feasible.

- Tue,  9 Aug: Hack up watching zone start/stop and vm start/stop. Grokking Riak models and indexing.
- Wed, 10 Aug: Test plan for zone/vm start/stop in COAL. Test cases. Clean up test suite.
  -> Today trent moved over to help with no.de for nodeKO. Hence, amon
     probably not making SDC 6.5.
- Thu, 11 Aug: [Nectarine] Email notification plugin (nodemailer). Amon-master and amon-relay builds (bamboo, mountain-gorilla).
- Fri, 12 Aug: Amon-master and amon-relay builds (bamboo, mountain-gorilla).
- Mon, 15 Aug: amon & amonriak headnode zones (using Jerry's headnode-refactor process)
- Tue, 16 Aug: [Trent on Lime-no.de upgrade.]
- Wed, 17 Aug: [Lime-no.de updated. Trent on Lime-no.de upgrade.] 
- Thu, 18 Aug: More headnode zones work. Upgrade plan. 
- Fri, 19 Aug: Finish upgrading impl.
- Mon, 22 Aug: [Trent DSAPI work.]
- Tue, 23 Aug: [Trent DSAPI work.]
- Wed, 24 Aug: [Trent vacation]
- Thu, 25 Aug: [Olive] Retry/backoff everywhere (restify). 
- Fri, 26 Aug: [Trent vacation]
- Mon, 29 Aug: Amon load testing and test suite improvements. Amon db garbage collection.
- Tue, 30 Aug:
- Wed, 31 Aug: [Trent vacation]
- Thu,  1 Sep: [cutoff day (morning)]
- Fri,  2 Sep: [Trent vacation]
- Mon,  5 Sep: [Public holiday (Labour day)]
- Tue,  6 Sep:
- Wed,  7 Sep:
- Thu,  8 Sep: [Papaya, SDC 6.5 release]

Workdays left: 14 days (starting at 9-Aug) - 2 days lost to no.de help (at least).



# TODOs for MVP

- Build/install (usb-headnode, bamboo, mountain-gorilla):
  - riak: discuss with others if separate riak clusters for separate apps
    or one shared one? billing/amon/hostrouter(?)/capi
  - master
  - relay (SMF packaging done; bamboo started)
  - agent
  - package.json: get specific about included versions
- Email Notification Plugin
- Retry/backoff everywhere: Restify-client supports everything needed here?
- event on zone/vm transitions: use zwatch
  - what does it mean to design to have events coming NOT from amon-agent
- Garbage collection of /events and orphans
- in riak, use uuid instead of customer_name for keys, if possible
- load testing (hook into mock cloud for this?)
  - estimate of design-limit on number of watches/notifications/events
  - e.g., what would it be on no.de
  - capacity planning:
    - http://wiki.basho.com/Cluster-Capacity-Planning.html
    - shared riak cluster with billing and ufds
    - http://wiki.basho.com/Bitcask-Capacity-Planning.html
- upgrade plan and testing
  - versioning (see ca/Makefile, and 1.0.0 instead of 6.1)
  - config handling


# Trent's scratch area

- upgrade deps/node and deps/npm versions to latest
- faking an event (calling Relay API):
    $ bin/amon-relay-api /events -X POST -d @-
    {"check":"joyent_whistle","status":"error","metrics":{"name":"amon:logscan","type":"Integer","value":2,"data":{"match":"i hear a tweet from someone"}}}
    ^D
- too much noise in the log: can't have the logs grow this fast. Ticket to
  not log normal "HEAD /config" hits?
- suppressing some zone transition events: How?
  - the start + reboot for zone provisioning:
    - talk with Orlando about a "ready" zoneconfig var set by provisioner
  - the stop/start of an intentional reboot (by an operator at the GZ
    command line)
    - sdc-amon-suppress ...   # or some command line that
  - intentional reboot or shutdown in adminui
    - either adminui or mapi's shutdown/reboot command would call Amon
      API to suppress?
    - or this is a separate action in adminui to manually suppress
  - intentional reboot or shutdown in portal/cloudapi
    - if MAPI above, then this is done.
  - intentional reboot of the *compute node* for maintenance by operator
    - sdc-amon-suppress command, and/or equivalent in adminui UI.
  - sdc-amon-suppress:
    - one shot?
    - toggle?
    - what is scope? Low-level: per zone name and per monitor name. Optionally
      just per monitor name for all applicable zones?
        "Suppress this monitor [until enabled again later]."
        "Suppress this monitor for machines X, Y and Z."
        "Suppress all monitors for machines X, Y and Z."
      Or is this only about suppressing *alarms*? I.e. only at the master-level.
      Yes.
        "Suppress alarms for monitors M and N [until enabled again later]."
        "Suppress alarms for monitor M for N for machines X, Y, and Z."
        "... for the next hour."
      general:
        "Suppress alarms
          for monitors M and N (or all)
          for machines X, Y and Z (or all)
          for a certain amount of time (or until re-enabled)."
        
- too much noise in the (master) log: can't have the logs grow this fast.
  Ticket to not log normal "HEAD /config" hits?
  Perhaps a flag set on the response object that the post-middleware logger
  uses to suppress.
- running on smartos global zone:
  devrun.sh: USE_ZSOCK=1
- /config -> /zonechecks
- use "/customers" prefix instead of "/pub" for Master API.
- riaktag for contacts_customers:
    $ r /riak/contacts_customers/joyent
    HTTP/1.1 200 OK
    X-Riak-Vclock: a85hYGBgymDKBVIszHNmG2YwJTLmsTK84Nt2nA8qzMpqnwoVPsUPFM4CAA==
    Vary: Accept-Encoding
    Server: MochiWeb/1.1 WebMachine/1.7.3 (participate in the frantic)
    Link: </riak/contacts/joyent_trent>; riaktag="_", </riak/contacts/joyent_mark>; riaktag="_", </riak/contacts_customers>; rel="up"
    Last-Modified: Tue, 09 Aug 2011 20:08:42 GMT
    ETag: "4RLm0K8wDfGytxwFX4E3FQ"
    Date: Tue, 09 Aug 2011 20:08:53 GMT
    Content-Type: text/plain
    Content-Length: 1
  Don't like "_" for riaktag. Perhaps "contact"? Or does re-used of "_" make
  the Entity class easier? With separate riaktag we could probably
  have *one* "customers" bucket with links to contacts, events, etc. Tho,
  perhaps that is more expensive.
- bucket_prefix config value ("amon_") for shared riak usage.
  


# Post-6.5 TODOs and RFEs

- amon-agent in all smartos zones
- *trying* with a second riak node
- CA Agent Plugin
- HTTP Agent Plugin
- revive twillio notification backend as "sms"
- Default to CAPI lookup user email on no contacts
- missing data alarms in master: ???
- edge-triggered alarms in master for checks: ???
- /events cleanup (look like other master apis)
- client SDK
- s/joyent/sdc/ in various runtime files and dirs
- Admin UI
- amon client
- snmp (MarkC: "snmp traps". Me: What's that?)
