# higher-level plan

This is meant to capture all the major pieces of required with for Amon
shipping in SDC7. The ultimate authority on details is "MON" jira tickets
assigned to the SDC7 release, but this gives an overview.

Items marked with "[now]" are for Trent to do before moving on to mainly
working on IMGAPI et al.


- [now] MON-113: maintenance windows
- [now] MON-114: disabling of probes and probegroups
- [now] MON-177: remove 'suppression' on alarms (use maint windows instead)
- [now] MON-178: doc review: clean up use cases, purge 'monitor'
- [now] MON-179: plan for more common usage/dogfooding/utility within dev group
- [now] MON-120: authorizeDelete finish (should merge with authorizePut logic)
- [now] MON-21: finish CRUD on UFDS objects (mainly update/PUT)
- [now] MON-108: clean up error story (verror, spec)
- [now] ticket triage
- [now] demo with ops people and feedback
- [kevin] adminui integration
- [risk-high] agent release process for running in customer zones
- MON-28: sdc backup/restore/upgrade for Amon
- MON-24: sdc-healthcheck
- MON-150: contacts refactor
- amon-relay heartbeating to detect whole CN going down
- notification content: spec webhook payload, email clean up and make
  more useful
- cloudapi integration
- portal integration
- alarm re-notification (see section in design.restdown)
- [risk-high] amon-agent re-write in non-node
- load testing. Estimate of design-limit on number of
  probes/notifications/events
- HA
- upgrade plan: What if UFDS schema needs to change? How do we rev the Master
  with older agents in the wild. Upgrading Agent versions? What about
  an old amon-agent? old amon-relay?



# v2 items (i.e. post-SDC7)

- amon-agent for non-SmartOS



# todo

In no particular order, other things to look at in Amon.

- smf service probe type, with clear support
- bitrot: amon-master's manual ldap pooling: use ufds.js
- update to latest ldapjs for #49 and #50 fixes (already done I think)
- ldapjs with pooling when that is ready.
- fix, update, use and test relay/lib/app.js#sendOperatorEvent
- update to and use npm shrinkwrap
- removing old alarms after a week
- how to monitor percentiles? E.g. prio1 alarm if 90th percentile
  response time is >200ms or something?
- bitrot: node_redis v0.8 (does this fix some of the problems we worked around?)
- Relay needs to have guards for run away agents. One case is an infinite
  loop: a probe watching for "ERROR" in amon logs, where sending the
  event results in an "ERROR". Handle this with relay per-agent throttling
  (can't use restify built-in throttling for this). If over limit, then
  an alarm is created for the monitor owner and the relay ignores or
  closes that zone server/socket.
- Consider adding run-time as attribute on probe events, i.e. how long it
  took the probe to run. Also, send a warning config alarm to a monitor owner
  if a probe is taking too long to run, e.g. a pathological regex. A *really*
  pathological regex (or mdb command) could hang and would need process
  mgmt to deal with. How to break out of this??? A separate process everytime
  is a pain.
- limits on number of objects per user: 1000 maint windows, 1000 monitors,
  1000 probes
- design use cases, assign a use case driver for each prio
    - use case #3: needs "smf" probe type to watch smf state transitions
      if possible
- hit up Ben for some "best practices for perf monitoring" info
  per https://mail.google.com/mail/u/1/?ui=2&shva=1#inbox/132dfa4e26d17478
- clear event scenario: A monitor with 'machine-up' probes on two zones.
  Both go down. One comes back up. That alarm should not clear, **but it
  does currently**.
- node-retry in PutEvents in RelayClient.
- What if a whole CN goes down? Could have heartbeats from the relays. Then
  have a probetype that fires for "missing" data. I.e. missed a heartbeat
  from the relay. Should this "fire on missing data" be a generic probe
  config thing? Probably.
  See <http://circonus.com/resources/videos#play-video-rule-notifications>
  for generic probe (aka rule) facilities: on match, invert match, on
  value change, on absence.
- send an op notification if Redis is down when attempting to process an event
- walk through a server boot (and boot of all its zones): does Amon eventing
  go crazy?


# someday/maybe

- bitrot: mark prefers node-backoff to node-retry
- long stack traces: https://github.com/mattinsler/longjohn
- nicer notification email formatting. Apple Store example: (trent's)
  https://mail.google.com/mail/u/1/?ui=2&shva=1#inbox/137c7a2c41219cee
    - see https://github.com/niftylettuce/node-email-templates and
      https://github.com/LearnBoost/juice
- more notification types: (see http://circonus.com/why-us/features/notifications)
    AOL and XMPP (Jabber, Google Talk, etc.) Instant Messages
    Twitter (direct messages from @circonusops)
    SMS
    PagerDuty
- http://circonus.com/resources/videos#play-video-rule-notifications for
  probe facilities.
- probe types to consider: https://support.cloudkick.com/Category:Checks
  https://nodefly.com/
- endpoint to test sending to a contact
- restify v2.x
- "sdc-amon /pub/trent.mick" messes up on '.' in username -> restify upgrade
  Check for this being fixed with restify 1.x
- from master/lib/probes.js:
    //XXX validate the type is an existing probe type
    //XXX validate data for that probe type
- [Trent] :login -> :uuid
- docs: move contact URN spec from master/lib/contact.js to the docs
- Idea: have amon-master create event/alarm to operator if it cannot
  connect/work with UFDS.
- look at New Relic: http://newrelic.com/
- test Amon on CN
- 'ping-agent' probe type. Basically this is a test that the amon-relay <->
  amon-agent socket is working and that the agent is responding.
- sigar
- from workflow/mapi talk: Amon should have the monitor skel for provisioning
  workflow a la FWAPI (but still use normal Amon comm channel). Spec that.
- check interval: cloudkick checks run every 2.5 minutes.
- use case: modcloth request to have alerts when in CPU bursting range, i.e.
  they are hitting OS caps. In Modcloth's case, they were hitting caps and
  didn't know it.
- amon-relay just fails mostly silently if given agents-probes-dir doesn't exist:
      [15:40:37 trentm@banana:~/joy/amon/relay (ufds)]
      $ ../deps/node-install/bin/node main.js -v -d -D tmp/db -m http://127.0.0.1:8080 -s 8081 -p 90
      2011-11-10 23:40:43Z DEBUG: Checking master for new agent probes.
      2011-11-10 23:40:43Z DEBUG: Starting new amon-relay for global zone at "8081" (owner=joyent).
      2011-11-10 23:40:43Z WARN: unable to create staging area tmp/db/global: Error: ENOENT, No such file or directory 'tmp/db/global'
      2011-11-10 23:40:43Z DEBUG: Starting app on port 8081 (developer mode)
      2011-11-10 23:40:43Z INFO: Amon-relay listening in global zone at 8081.
      2011-11-10 23:40:43Z WARN: Unable to save new agent probes: Error: ENOENT, No such file or directory 'tmp/db/.6d617f32-4062-4bab-aa15-dfcf8f9e7113'
      2011-11-10 23:40:43Z INFO: Successfully updated agent probes from master (zone: global, md5: undefined -> UaTV7YW3MTyNMX+uguZZZw==).
- review <https://hub.joyent.com/wiki/display/dev/Operating+a+server+fleet>
  How are we doing?
- review and answer MVP points
- JSON api summary at "GET /"
- HTML docs at "GET /docs"
- SNMP traps (per Mark).
  http://tools.ietf.org/html/rfc3877  Alarm MIB
  http://tools.ietf.org/html/rfc3014  Notification Log MIB
  Traps not in snmpjs yet. Keith: """you can however use snmptrapgen, which
  comes with Net-SNMP on our systems it's just a binary program that
  generates traps and directs them somewhere of your choosing. So you can see
  what they look like. That may help you start writing the MIB you want to
  use (definitions of data objects, types, and structures)"""
- https://github.com/davepacheco/kang -ify
- edge-triggered alarms in master for probes
- CA probe type
- probe types: http://circonus.com/why-us/features/monitoring
- compare to http://www.rackspace.com/blog/cloud-monitoring-early-access-program-opens-its-doors/
  http://docs.rackspace.com/cm/api/v1.0/cm-devguide/content/index.html
- revive twilio notification backend as "sms"
- more probe types: run cmd, http check, zone list, ping, disk free, disk
  iops, cpu, mem, ps (aka "top")
    - http://blog.nodeping.com/2012/03/28/ssl-certificate-check/
- torture test on amon-relay resiliency on start
    // XXX Test/design for race condition where: get list of zones, zone
    //     goes down before zsock is created for it. I.e. should be buffering
    //     zoneevents until completed initial setup.
  Amon-relay restarts with lots of zones, while chaos-monkey hup'ing zones.
  Run that for a while then ensure that none of the zones are fubar'd with
  failures to shutdown.
- Interesting services for hooking into, messaging services that might be useful,
  or just general related/competing services:
    - http://aws.amazon.com/cloudwatch/
    - https://www.cloudkick.com/
    - http://www.splunk.com/
    - http://www.pagerduty.com/
    - http://www.opennms.org/ (modcloth using this I think)
    - circonus.com
    - newrelic


# notes: redis

- TODO: test redis being down and down/up/down/up frequently: does that break
  amon-master? It shouldn't.
- TODO: Test a notification going through even if redis is down.
- redis zone config: have "save 60 1". Make sure that isn't filling up disk
  with lots of snapshots. I *presume* this is just one or two files on
  disk, but should check.
- TODO: test how events are handled when redis runs out of memory
  Might want "maxmemory" and "maxmemory-policy ..." or "vm-enabled yes" in
  redis.conf

Ensure do "SELECT 1" (or some value of N) to stay indep of other users of
redis. TODO: Come up with a system for sharing this out. Suggest not using 0
such that any usage of it is an error to trap faulty users.



# Notes: suppression

(Note: Mostly this is, or should be, superceded by maintenance window
support. See design.restdown section for that.)

- suppression facility: global, per monitor, per customer? "per customer"
  could be implemented client side: portal/adminui just sets it for all
  that customer's monitors (looses mixed suppression states).

- suppressing some zone transition events: How?
  - the start + reboot for zone provisioning:
    - talk with Orlando about a "ready" zoneconfig var set by provisioner
  - the stop/start of an intentional reboot (by an operator at the GZ
    command line)
  - intentional reboot or shutdown in adminui
    - either adminui or mapi's shutdown/reboot command would call Amon
      API to suppress?
    - or this is a separate action in adminui to manually suppress
  - intentional reboot or shutdown in portal/cloudapi
    - if MAPI above, then this is done.
  - intentional reboot of the *compute node* for maintenance by operator
    - sdc-amon-suppress command, and/or equivalent in adminui UI.
  - sdc-amon-suppress:
    - one shot?
    - toggle?
    - what is scope? Low-level: per zone name and per monitor name. Optionally
      just per monitor name for all applicable zones?
        "Suppress this monitor [until enabled again later]."
        "Suppress this monitor for machines X, Y and Z."
        "Suppress all monitors for machines X, Y and Z."
      Or is this only about suppressing *alarms*? I.e. only at the master-level.
      Yes.
        "Suppress alarms for monitors M and N [until enabled again later]."
        "Suppress alarms for monitor M for N for machines X, Y, and Z."
        "... for the next hour."
      general:
        "Suppress alarms
          for monitors M and N (or all)
          for machines X, Y and Z (or all)
          for a certain amount of time (or until re-enabled)."


# notes: fma

Use case: operator wants an alert for every zone fault.

- FMA-based check in amon-agent in all CN GZs
  Q: Ask rm how to trigger that FMA event.
- Q: Exclude explicit reboots? What about an explicit reboot, but it doesn't
  come back up?
  A: Yes, exclude for now.
- Q: how to handle clearing an alarm. Does FMA give me an event for that?
  Ditto for non-fma-based zone status watching? Are we going to send an
  amon event for *every* zone running state every minute? No way.


# notes: smartos event streams notes

- sysevent: core element, syseventadm
  https://mo.joyent.com/source/xref/illumos-joyent/usr/src/lib/libsysevent/libsysevent.c#2653
    sysevent_subscribe_event
  Note that zonecfg_notify_* itself calls a slightly different "sysevent_evc_subscribe":
    https://mo.joyent.com/source/xref/illumos-joyent/usr/src/lib/libsysevent/libevchannel.c#566
    https://mo.joyent.com/source/xref/illumos-joyent/usr/src/lib/libzonecfg/common/libzonecfg.c#6692
- zonecfg_notify_bind: uses sysevent, might not need that
  defined here: illumos/usr/src/lib/libzonecfg/common/libzonecfg.c
- fma:
  fmdump
  FMA: http://hub.opensolaris.org/bin/download/Community+Group+fm/WebHome/FMDPRM.pdf
  http://download.oracle.com/docs/cd/E19082-01/819-3196/6n5ed4h40/index.html#indexterm-289
  fminject to test


# notes: events

restore this original sorta-schema for events from Mark?
The (sort of) schema for sending a status update:

      {
        "status": "<STATUS>",
        "message": "Free Form String.",
        "metrics": [{
          "name": "<URN>",
          "type": <TYPE>",
          "value": <VALUE>
        }];
      }

Where:

* <STATUS>: A string, that must be one of `"ok", "warn", "error"`.
* <URN>: The name of the check (note this will be validated against :check).
  An example is something like `urn:cpu:load`.
* <TYPE>: One of `"String", "Integer", "Boolean", "Float"`
* <VALUE>: Value for this metric.  Must correspond to `type`.

If not obvious, you can send multiple metrics per check.


# notes: maintenance windows

- regrok: alarm.handleEvent, app.handleMaintenanceEnd,
  maintenances.apiListAllMaintenanceWindows
- add app.handleMaintenanceEnd
    - add 'reason' code field to `alarm.notify(...)` for the notification
      types to handle different reasons
- tests for maintenance windows
- separate commit: replace SuppressAlarmNotifications and UnsuppressAlarmNotifications
  with a maint window on that alarm's monitor
- XXX's in maintenances.js
- fix this app crash (when that alarm id is a 404):
    [root@headnode (bh1-kvm7:0) ~]# sdc-amon /pub/deadbeef-4444-4444-4444-444444444444/alarms/2 -X DELETE
    curl: (52) Empty reply from server
  traceback:
    node.js:201
            throw e; // process.nextTick error, or 'error' event on first tick
                  ^
    TypeError: Cannot set property 'faults' of null
        at /opt/smartdc/amon/lib/alarms.js:277:23
        at /opt/smartdc/amon/node_modules/redis/index.js:996:13


# notes: dogfoog

- devs using in kvm test machines
- devs using in COAL
- trevor using it in BH-1. For what exactly?
- monitoring jenkins builds? or space usage in jenkins?
- space usage on stuff
- is DSAPI up?
- is mo.jo up?
- others? email around
